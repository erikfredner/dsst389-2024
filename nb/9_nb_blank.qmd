---
title: "Sentiment Analysis"
subtitle: "DSST389: Advanced Data Science"
author: "Erik Fredner"
institute: "University of Richmond"
date: today
execute:
echo: true
  warning: false
format:
  html:
    anchor-sections: true
    code-tools: false
    code-link: true
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    number-sections: true
    smooth-scroll: true
    toc: true
editor:
  markdown:
    wrap: 72
---

## Environment

Run the following in a code chunk or your console to clean up your environment:

```r
if (interactive()) {
  rstudioapi::restartSession(clean = TRUE)
}
```

You may need to install some new libraries and their dependencies:

```{r}
packages <- c(
  "tidytext", "gutenbergr", "janeaustenr", "textdata",
  "tidyverse", "ggrepel", "scales"
)

for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}
```

Then, import needed libraries:

```{r}
#| label: libraries

for (pkg in packages) {
  library(pkg, character.only = TRUE)
}

theme_set(theme_minimal())

set.seed(123)
```

## Data

Today, we're going to work with two data sets of tweets. (Yes, "tweets"---the data predates Twitter becoming X.) The data is from [here](https://github.com/zfz/twitter_corpus).

### Posts

```{r}
posts <- read_csv("../data/tech_twitter_posts.csv")
```

`posts` contains the following columns:

| Column | Data Type   | Description                                                           |
| ------ | ----------- | --------------------------------------------------------------------- |
| `id`     | Number      | Unique numeric identifier for each tweet (a long number).            |
| `dttm`   | [Date-time](https://r4ds.had.co.nz/dates-and-times.html#creating-datetimes)   | The date and time when the tweet was posted.                         |
| `topic`  | Text        | A human-assigned topic for the tweet. This data set contains tweets about tech companies (e.g., "apple").                     |
| `text`   | Text        | The text content of the tweet.                               |

### Post sentiments

```{r}
post_sentiments <- read_csv("../data/tech_twitter_sentiments.csv")
```


`post_sentiments` contains the following columns:

| Column            | Data Type | Description                                                    |
|------------------ |----------|----------------------------------------------------------------|
| `id`               | Number    | Unique numeric identifier for each tweet (matches `posts$id`). |
| `sentiment_manual` | Text      | Human-coded sentiment of the tweet (e.g., "positive", "negative", "neutral"). |

## Analysis

### Goals

We have two primary goals with this analysis:

1. Compare the sentiments expressed in this data set about each of the tech companies referenced.
2. Compare human-created sentiment labels to automatic sentiment labels using various sentiment analysis data sets.

### Tokenization

Tokenize the `text` of the tweets. Once you are sure you have done this correctly, create a new variable called `tidy_posts` with the tokenized data, then sample 5 random rows from `tidy_posts`.

```{r}
#| label: q-tokenize
```

### Exploratory data analysis

Using the `afinn` lexicon, calculate the length and net sentiment scores for each tweet. In cases where *none* of the words in the tweet are in the `afinn` lexicon, assign a sentiment score of zero. Finally, calculate a normalized `afinn` score that accounts for the total length of the tweet. Check your results against the tweet with the following `id`:

```r
id == 126188946974720000
```

```{r}
#| label: q-afinn
```

Using the pipeline above, visualize net sentiment scores per tweet by topic using [`geom_jitter`](https://ggplot2.tidyverse.org/reference/geom_jitter.html) and [`geom_boxplot`](https://ggplot2.tidyverse.org/reference/geom_boxplot.html).

```{r}
#| label: q-boxplot
```

Based on the chart, it appears that the normalized means and medians are quite similar across topics. Is that true?

Calculate summary statistics to match the table below to check. (`q25` and `q75` are calculated using [`quantile()`](https://search.r-project.org/R/refmans/stats/html/quantile.html).)

```{r}
#| label: q-summary
```

What does this table clarify that we were not able to see in the boxplot?

### Evaluating against original tweets

Let's see how well these findings match our intuitions. Return the topic and the original tweet text for the top three most positive tweets about each topic.

```{r}
#| label: q-positive-tweets
```

Read the text of each tweet, and assess the accuracy of the normalized `afinn` score qualitatively. What do you observe?

### Tweets over time

The `dttm` column in `posts` contains the date and time of each tweet. Let's plot a histogram of the net sentiment of tweets about Apple over time using the `afinn` lexicon. We are going to create one hour bins of time and calculate the net sentiment of tweets in each bin.

To do this, we are going to take advantage of [the `floor_date()` function](https://lubridate.tidyverse.org/reference/round_date.html), which rounds a date-time object to the nearest unit of time:

```r
floor_date(dttm, unit = "1 hour")
```

```{r}
#| label: q-time-series
```

### Automatic vs. human labels

Now, we are going to evaluate these `afinn` labels against human-created labels for each of these documents.

`post_sentiments` contains three manual sentiment classifications: `"positive"`, `"neutral"`, and `"negative"`.

Using [`case_when()`](https://dplyr.tidyverse.org/reference/case_when.html), create categories for your normalized `afinn` scores. Define a tweet as "positive" or "negative" if its normalized score is more than one standard deviation greater or less than the mean normalized `afinn` score.

```{r}
#| label: q-make-classes
```

How well dispersed are these classes within each topic? Create a stacked bar chart that shows the proportions of tweets by topic and class.

:::{.callout-tip}
If you want to match the y-axis on my example plot, try this line:

```r
scale_y_continuous(labels = percent_format())
```
:::

```{r}
#| label: q-stacked
```

Now, how often do these `afinn` classifications match the human-created labels that we have in `post_sentiments`? To test this, we are going to use `geom_tile()` to create what is called a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). For the confusion matrix, we will compare the "true" human-created values to the sentiment classification model we created above.

Each of the squares in the confusion matrix should indicate how many posts were classified in each category. If `post_sentiments` and our model were perfectly aligned, there would only be values in three of the nine squares in the confusion matrix.

```{r}
#| label: q-confusion
```

If you have not seen a confusion matrix before, spend some time looking at this to ensure that you understand what it means. In cases where the manual and `afinn` classes match (e.g., the positive-positive intersection), both models classified the same tweet in each way. In mismatched cases, (e.g., manual says positive but `afinn` says netural), that indicates that the model *erred* relative to the human label. It is a "confusion" matrix because it quantifies where and how the model gets confused.

This is another way of visualizing what we saw in the bar plot above: Most posts are relatively neutral, and the model gets that mostly correct. But, for both positive and negative tweets, the `afinn` model only performs a bit better than random chance among the three categories (and, in some instances, *worse* than it would do if the classes were weighted based on the human-made labels).

### Comparing sentiment methods

Let's compare the `afinn` results to another sentiment lexicon, `bing`.

First, get the `bing` sentiments, then use them to calculate net sentiment scores normalized by the length of each tweet.

```{r}
#| label: q-setup-bing
```

Next, let's recreate the stacked bar plot from above using the `bing` sentiments, and compare to `afinn`:

```{r}
#| label: q-stacked-bar-2
```

Finally, let's recreate the confusion matrix and see how `bing` performs compared to `afinn`.

```{r}
#| label: q-bing-confusion
```

Which of these sentiment lexicons performed better? How do you know? What differences do you observe between the matrices? (Hint: Look at the integers.) What could explain the differences across the two lexicons?

### Extra challenge

Above, you were asked to evaluate the performance of the sentiment lexicons by looking at the confusion matrices. But you could also perform this comparison computationally. Quantify the relative performance of `afinn` and `bing` on this task, using the human-created labels for the tweets as the data to evaluate whether the model got the label "right."

## Summary

This notebook practices...

- **Setting up an R environment** (installing/loading packages, restarting sessions)  
- **Reading and cleaning real-world text data** (tweets)  
- **Tokenizing text** with the `tidytext` workflow  
- **Applying dictionary-based sentiment analysis** (AFINN and Bing lexicons)  
- **Comparing manual and automatic sentiment labels** (visualizing and interpreting results)  
- **Creating time series visualizations** (histogram)
- **Data visualization techniques** with `ggplot2` (box plots, jitter plots, bar charts, and confusion matrices)  
- **Aggregating and summarizing data** (grouping, summarizing, and calculating descriptive statistics)
